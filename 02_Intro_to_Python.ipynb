{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "If you want to store information and the order doesn't matter, but you want to look it up with a key, then you should use a dictionary (indicated with {} curley brackets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = {\"Paul\":34, \"Padma\":25, \"Gorp\":155}\n",
    "\n",
    "print(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer will not rember this order! To get information out, you can use the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gorpage = ages[\"Gorp\"]\n",
    "\n",
    "print(gorpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add data to a dictionary easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages[\"Blobl\"] = 20\n",
    "print(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Natural Language Toolkit\n",
    "The natural langauge toolkit is a library in Python that contains a large number of functions and materials that will be useful to linguists and anyone interested in using Python to study natural language documents. Today we will introduce the basic functionalities of the library, including using the built in corpora (there are many of them), using concordances, preparing a collection of texts you found for use with NLTK, lemmatization, part of speech tagging, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To use NLTK, you will need to import the library at the top of your script\n",
    "import nltk\n",
    "# The first time you will need to run \n",
    "# nltk.download() to download extra materials.\n",
    "# just download the book materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Included Corpora\n",
    "If you have downloaded the extra information from NLTK, it includes many useful corpora. We will explore some of them here, so you know what is available. It is also possible to load your own corpora to use NLTK functionality, which we will discuss in a moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg Sample\n",
    "Project Gutenberg is a project that makes many public domain texts freely available. You can check it out at www.gutenberg.org\n",
    "Some of these are preloaded in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shakespeare Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, NLTK includes over 100 corpora prepackaged. You can find the full list at\n",
    "http://www.nltk.org/nltk_data/\n",
    "\n",
    "## Creating an NLTK Text Object\n",
    "In practice, you will probably need to create your own NLTK text object to be able to use all of the NLTK functions easily. This is relatively straigthforward.\n",
    "\n",
    "### Initial Cleaning\n",
    "In order for the computer to do much with a document, it will need to be cleaned and then tokenized.  \n",
    "\n",
    "We will first load a text and then clean it up a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a text stored in this directory\n",
    "text = open(\"holmes.txt\",\"r\").read()\n",
    "\n",
    "# Print the first 200 characters to be sure that it has loaded properly\n",
    "print(text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We don't want this preamble, so we will get rid of it (and the text at the end of the document)\n",
    "# Most gutenberg ebooks contain some boilerplate that ends with\n",
    "# *** START OF THIS PROJECT GUTENBERG EBOOK\n",
    "# The final boilerplate starts with\n",
    "# *** END OF THIS PROJECT GUTENBERG EBOOK\n",
    "# So we will search for these two phrases and get rid of them:\n",
    "text = text[text.find('*** START OF THIS PROJECT GUTENBERG EBOOK'):text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')]\n",
    "\n",
    "print(\"Start of text\",text[0:100])\n",
    "print(\"End of text\",text[-100:])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "NLTK can break texts apart into individual sentences or words (or both, if you like).\n",
    "\n",
    "NLTK comes with a tokenizer that makes this easy (this works best in English, but there are many more tokenizers available online)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This breaks the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternatively, you can break the document into words:\n",
    "# Note that this considers punctuation marks \"words\"\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can do both at the same time to make a list of sentences, which are each a list of words\n",
    "sent_word_tokens = []\n",
    "for sentence in nltk.sent_tokenize(text):\n",
    "        tokenized_sent = nltk.word_tokenize(sentence)\n",
    "        sent_word_tokens.append(tokenized_sent)\n",
    "print(sent_word_tokens[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code can be condensed into a single line:\n",
    "sent_word_tokens = [nltk.word_tokenize(sentence) for sentence in nltk.sent_tokenize(text)]\n",
    "print(sent_word_tokens[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Text Object\n",
    "From here, creating the text object is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will generate the Text object with just the words, because\n",
    "# the functions are easier to use\n",
    "novel = nltk.Text(words)\n",
    "print(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Objects\n",
    "NLTK text objects contain many useful functions that will let you quickly visualize assertain information about the text. We will discuss several of the most useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a concordance for a specific term\n",
    "novel.concordance('Holmes', 100,        50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get similar words based on context\n",
    "# Note that the results will make better sense with a very large corpus\n",
    "novel.similar(\"jewel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dispersion plot with a list of words.\n",
    "# This will make visualizing relations easy\n",
    "# You can give it a single word, or a list of words\n",
    "novel.dispersion_plot([\"murder\",\"detective\",\"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can generate a list of collocations very easily\n",
    "novel.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information\n",
    "Sometimes we will want to get some basic information about a text. How long is it in characters, words, and sentences (you can see a description of this in http://www.nltk.org/book/ch02.html)? What is the lexical diversity (the ratio of unique words to total words)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the lenght in characters. This is VERY easy:\n",
    "charlength = len(text)\n",
    "print(charlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the length in words is slightly more complicated, because we don't really want to consider punctuation marks words (unless we do). As such, we will want to do a bit of editing first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First let's look at the contents of the \"words\" list:\n",
    "print(words[:100])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get rid of the punctuation marks and make everything lowercase:\n",
    "filtered_words = []\n",
    "# Go through each word in the words list\n",
    "for word in words:\n",
    "    # check if the \"word\" is alpha-numeric \n",
    "    if word.isalnum():\n",
    "        # if it is, append a lowercase version of the word to the filter words list:\n",
    "        filtered_words.append(word.lower())\n",
    "'''        \n",
    "This can be done in one line with:\n",
    "filter_words = [word.lower() for word in words if word.isalnum()]\n",
    "'''\n",
    "print(filtered_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now getting the number of words is straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = len(filtered_words)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting unique words is also easy. The \"set\" object type is a default Python object type and only allows one of each value (we will turn it back to a list for easy use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "for word in filtered_words:\n",
    "    if word not in unique_words:\n",
    "        unique_words.append(word)\n",
    "\n",
    "\n",
    "#unique_words = list(set(filtered_words))\n",
    "print(unique_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity\n",
    "Now that we have the length in words and the unique words, calculating the lexical diversity of the text is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexical_diversity = len(unique_words)/total_words\n",
    "print(lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length\n",
    "There are several ways to calculate this. One could, as is done in the NLTK book, simply divide the number of characters in the text by the number of words. However, remember that we got rid of punctuation, which will skew the results. To avoid this, we can use a loop to cacluate this informatin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_word_length = 0\n",
    "for word in filtered_words:\n",
    "    # add together the length of each word\n",
    "    total_word_length = total_word_length + len(word)\n",
    "avg_word_length = total_word_length/len(filtered_words)\n",
    "print(\"refined word length:\", avg_word_length)\n",
    "print(\"NLTK approach:\", charlength/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average sentence length\n",
    "Calculating average sentence length is done in much the same way (here we will use the sentence and word tokenized object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsinsentences = 0\n",
    "for sent in sent_word_tokens:\n",
    "    for word in sent:\n",
    "        if word.isalnum():\n",
    "            wordsinsentences += 1\n",
    "            \n",
    "avgsentencelength = wordsinsentences/len(sent_word_tokens)\n",
    "print(avgsentencelength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "You can quickly get word frequencies by using the FreqDist function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequencies = nltk.FreqDist(filtered_words)\n",
    "print(frequencies['and'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can see the most common words (or words that only occur once)\n",
    "#print(frequencies.most_common(50))\n",
    "print(frequencies.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and using Corpus objects\n",
    "Working with a single text is usfeul, but it is often more useful to deal with multiple texts. As mentioned at the start of this lesson, NLTK comes with a number of built in corpora, but it is just as easy to build your own corpus. Just create a folder that contains the texts you are interested in. I've provided a set of texts downloaded from the most popular section of Project Gutenberg and placed them in a folder called \"my_corpus\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# Just give the corpus reader your folder as the first parameter\n",
    "# and a regular expression that matches files of interest.\n",
    "# We don't have time to go into regular expressions, but I've provided one\n",
    "# That will simply match all files in the folder.\n",
    "my_corpus = PlaintextCorpusReader('my_corpus','.+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information from Corpus object\n",
    "Now that we have the corpus object, we can extract information from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a list of the files in the corpus\n",
    "print(my_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw gives you access to the text as a string (we will use frederickdouglas.txt)\n",
    "print(my_corpus.raw('frederickdouglas.txt')[40000:40050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words provides the words (this will be similar to the TextObject used above!)\n",
    "print(my_corpus.words('frederickdouglas.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sents provides the sentences\n",
    "print(my_corpus.sents('frederickdouglas.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The following example is taken from nltk.org/book/ch02.html.\n",
    "# print information for each text in the corpus\n",
    "\n",
    "# here you might run into an esoteric AssertationError relate to check1 vs check2\n",
    "# this appears to be bug in NLTK related to Byte Order Markers. Copying the offending\n",
    "# text directly from the source into a new file seems to fix it.\n",
    "for fileid in my_corpus.fileids():\n",
    "    if fileid != '.DS_Store':\n",
    "        num_chars = len(my_corpus.raw(fileid))\n",
    "        num_words = len(my_corpus.words(fileid))\n",
    "        num_sents = len(my_corpus.sents(fileid))\n",
    "        num_vocab = len(set(w.lower() for w in my_corpus.words(fileid)))\n",
    "        print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords:\n",
    "Often we do not care about the most common words in the corpus. NLTK comes with stopword lists in a variety of languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "my_function_words = [\"an\",\"of\"]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing these stopwords from a text is easy:\n",
    "no_stop_words = [word for word in filtered_words if word not in stopwords]\n",
    "print(no_stop_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the NLTK book recomends suggests calculating the content fraction\n",
    "print(len(no_stop_words)/len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming words in a document\n",
    "Often you will want to stem the words in your document, that way you don't have the 'whale' and 'whales' cluttering up your counts. Similarly, you may want \"runs\" and \"running\" to be counted with \"run\". NLTK comes with an extensive toolbox for doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the SnowballStemmer (one of the included stemmers)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# instantiate a stemmer object\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_words = []\n",
    "for word in filtered_words:\n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "    \n",
    "# or do this:\n",
    "# stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip stopwords when using the stemmer by adding a simple parameter when you instantiate the original stemmer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(stemmed_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "NLTK also includes a lemmatizer, which checks that the stem is actually a real word. This often provides better results, but it is slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmafinder = nltk.WordNetLemmatizer()\n",
    "lemmas = [lemmafinder.lemmatize(word) for word in filtered_words]\n",
    "print(lemmas[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "Some corpora built into NLTK are already tagged by Part of speech (such as the Brown corpus), but you may want to do this yourself. This works best at the sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tagged_sentences = []\n",
    "for sent in sent_word_tokens:\n",
    "    pos_tagged_sentences.append(nltk.pos_tag(sent))\n",
    "print(pos_tagged_sentences[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the sentence has been tagged with its part of speech. To (TO: to), smoke (VB: verb, base form), he (PRP: personal pronoun), answered (VBD: verb past tense).\n",
    "\n",
    "NLTK uses the Penn Treebank Project part of speech tags, which you can find listed here:\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
